{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will be performing some analysis with entity extraction. In particular, you will be looking at the Reuters corpus and trying to construct entity profiles of persons, organizations, and locations. This will require you to iterate through the documents in the Reuters corpus, parse them appropriately, extract entities, and then store the entities along with some surrounding text. Additionally, you will be looking for mechanisms to identify potential relationships between persons and locations.\n",
    "\n",
    "\n",
    "Throughout this you will need to use NLTK to access the corpus. At the same time, you will need to use an entity extraction system. You will want to use Spacy for named entity recognition.\n",
    "\n",
    "The basic idea is to build a knowledge base around the entities you will extract in the Reuters corpus. Normally, this would be a first step to trying to model such things as entity resolution across documents. You could also use this as a first step to analyzing the sentiment towards particular entities. For example, people expressing dissatistfaction at a restaurant or brand.\n",
    "\n",
    "\n",
    "Follow the below steps and read the comments carefully on the types of tasks your code will need to do.\n",
    "\n",
    "\n",
    "I would expect that some of you might be able to reuse parts of this code for your project..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1) \n",
    "Import necessary librariesÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the corpus we work from\n",
    "\n",
    "# in order for this to work you will have to have installed NLTK \n",
    "# and also installed the reuters data\n",
    "\n",
    "# to install NLTK, pip install nltk\n",
    "# \n",
    "#\n",
    "# To install the reuters corpus following the instructions here: https://www.nltk.org/data.html\n",
    "# The easy way to install the Reuters corpus is usally:\n",
    "# import nltk\n",
    "# nltk.download('reuters')\n",
    "\n",
    "\n",
    "# This will import the Reuters corpus, assuming you have it\n",
    "from nltk.corpus import reuters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will want to use Spacy as your entity recognizer\n",
    "# my suggestion would be to make sure you are using a 2.x version of Spacy\n",
    "# pip install spacy==2.3.5\n",
    "import spacy\n",
    "# note, the model load can be odd. In some instances your model might have the full name or the short name here.\n",
    "# if you run into issues here, check the spacy model page at https://spacy.io/usage/models\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "# alternatively try: \n",
    "# spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2) \n",
    "FIll in the following function to extract the entity, document id, and relevant sentence text from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(doc_id, doc_text):\n",
    "    analyzed_doc = nlp(doc_text)\n",
    "    \n",
    "    # these two dictionaries will include all the persons and locations you find in a document.\n",
    "    # You will need to add each person or location you encounter in the document to them\n",
    "    # for the key you can use the text of the entity, for the value you will want to use the document_id and the\n",
    "    # text of the sentence one challenge could be that an entity might occur multiple times in the document, \n",
    "    # thus the value should really be a document id and a list of the text of the sentences ( or something such as that)\n",
    "    doc_persons = {}\n",
    "    doc_organizations = {}\n",
    "    doc_locations = {}\n",
    "    \n",
    "    for entity in analyzed_doc.ents:\n",
    "        if entity.text.strip() != \"\":\n",
    "            # The .label_ property will provide information on the type of entity tagged\n",
    "            print(\" -> \", entity.label_)\n",
    "            # The .text property will display the actual text of the entity in the text\n",
    "            print(\"->\", entity.text.strip(), \"<-\")\n",
    "            # You can also access the sentence that the entity is contained in by using the .sent property\n",
    "            # inside the sentence you can then use the .text property\n",
    "            print(\"->\", entity.sent.text, \"<-\")\n",
    "            \n",
    "            \n",
    "            # one way to represent the document id and the sentence text would be with a tuple\n",
    "            # thus, you could do:\n",
    "            relevant_sentence = (doc_id, entity.sent.text)\n",
    "            \n",
    "            # add the relevant document id and sentence to the entity record\n",
    "            \n",
    "            \n",
    "            \n",
    "    return doc_persons, doc_organizations, doc_locations\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3) \n",
    "Adjust the following code to run the document entity extraction function\n",
    "Also, add the entity records you are constructing to your master list of entities\n",
    "Note: for the full subission run across all the Reuters documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(reuters.fileids())\n",
    "#  this has a large number of files... \n",
    "# you might wish to limit the number of documents you use while developing your technique \n",
    "# ex. reuters.fileids()[0:25]\n",
    "\n",
    "# these two dictionaries will incorporate all the referneces to \n",
    "combined_persons = {}\n",
    "combined_organizations = {}\n",
    "combined_locations = {}\n",
    "\n",
    "# this will only iterate over the first 25 documents, for the real submission you will need to run across all documents\n",
    "for doc_id in reuters.fileids()[0:25]: \n",
    "    # this doc_text variable will give you a text version of the news article. This could be tokenized.\n",
    "    persons, organizations, locations = extract_entities(doc_id, reuters.open(doc_id).read())\n",
    "    \n",
    "    # you will need to write something here to put the persons and locations found in a document into the \n",
    "    # combined_persons, combined_organizations, and combined_locations dictionaries.\n",
    "    # here you will need to consider how to extend the values already in the dictionaries\n",
    "    # maybe something like:\n",
    "    # for person in persons.keys():\n",
    "    #     if person not in combined_persons.keys():\n",
    "    #         --- add a person key to the combined persons list\n",
    "    #     now here you can add the person's document ids and sentence texts to the dictionary value\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4)\n",
    "Fill in the following method to look through the content of an entity dictionary to determine the most popular based on number of mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have the text associated with the entities, \n",
    "# you will want to focus on the 500 top entities in each category\n",
    "# Identify the top 500 entities by the count of their occurrences\n",
    "def find_most_popular_entities(entity_dictionary):\n",
    "    # sort through the entities in the dictionary by the number of sentences\n",
    "    \n",
    "    return list_of_dictionary_keys_with_most_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5)\n",
    "Now invoke your top entity mention finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply get the top persons and locations\n",
    "top_persons = find_most_popular_entities(combined_persons)\n",
    "top_locations = find_most_popular_entities(combined_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6) \n",
    "\n",
    "Analyze the most popular entities to determine what words they most frequently occur with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these dictionaries to store the most frequent terms associated with the entities\n",
    "person_most_popular_terms = {}\n",
    "organization_most_popular_terms = {}\n",
    "location_most_popular_terms = {}\n",
    "\n",
    "# finally, now find the most frequent tokens associated with the entities\n",
    "for person in top_persons:\n",
    "    # fill this dictionary with all the words in the context of the person entity\n",
    "    person_token_dictionary = {}\n",
    "\n",
    "# finally, now find the most frequent tokens associated with the entities\n",
    "for organization in top_organization:\n",
    "    # fill this dictionary with all the words in the context of the person entity\n",
    "    organization_token_dictionary = {}\n",
    "\n",
    "    \n",
    "    \n",
    "for location in top_locations:\n",
    "    # fill this dictionary with all the words in the context of the location entity\n",
    "    location_token_dictionary = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7)\n",
    "\n",
    "Present your results of the most popular entities and their associated terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# present you results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "There are several extra credit options for this assignment.\n",
    "The first would be to determine which persons, organizations, and locations most frequently occur in the same sentences.\n",
    "Another task would be to attempt to resolve different forms of the same name for each person and location. For example, George Bush and Bush inside the same document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
